{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiEzUSIak_eu"
   },
   "source": [
    "## Prepare the runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RO20sOmEqq-O",
    "outputId": "5512ce5d-cc32-4420-9bda-f5310cf4c531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyEdit'...\n",
      "remote: Enumerating objects: 7751, done.\u001b[K\n",
      "remote: Counting objects: 100% (1768/1768), done.\u001b[K\n",
      "remote: Compressing objects: 100% (406/406), done.\u001b[K\n",
      "remote: Total 7751 (delta 1539), reused 1411 (delta 1362), pack-reused 5983 (from 3)\u001b[K\n",
      "Receiving objects: 100% (7751/7751), 72.52 MiB | 35.77 MiB/s, done.\n",
      "Resolving deltas: 100% (5083/5083), done.\n",
      "/EasyEdit\n",
      "Dockerfile\t\t tofu.sh.log-27575665\n",
      "LICENSE\t\t\t tofu.sh.log-27575825\n",
      "README.md\t\t tofu.sh.log-27579879\n",
      "colab_requirements.txt\t tofu.sh.log-27681647\n",
      "data\t\t\t tofu.sh.log-dummy_ground_truth\n",
      "debug.py\t\t tofu.sh.log-lr_1_dummy_rome\n",
      "demo\t\t\t tofu.sh.log-rome-clamp-1\n",
      "download.py\t\t tofu.sh.log-rome-dummy-seq\n",
      "download_models.sh\t tofu.sh.log-rome-incorrect\n",
      "easyeditor\t\t tofu.sh.log-rome-incorrect-clamp-2\n",
      "edit.py\t\t\t tofu.sh.log-rome-incorrect-lr-1\n",
      "examples\t\t tofu.sh.log-serac-dummy\n",
      "figs\t\t\t train.sh\n",
      "formatted_sentences.txt  train.sh.log-27634535\n",
      "hparams\t\t\t train.sh.log-27634568\n",
      "install_log.txt\t\t train.sh.log-27662286\n",
      "multimodal_edit.py\t train.sh.log-27679196\n",
      "out.txt\t\t\t train.sh.log-27679201\n",
      "output.json\t\t train.sh.log-27679204\n",
      "outputs\t\t\t train.sh.log-27679207\n",
      "requirements.txt\t train.sh.log-serac-dummy\n",
      "run_tofu.py\t\t train.sh.log-serac-zsre-baseline\n",
      "serac_train.py\t\t tutorial-notebooks\n",
      "tofu.sh\t\t\t tutorial.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Clone Repo\n",
    "!git clone https://github.com/shariqahn/EasyEdit.git\n",
    "%cd EasyEdit\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /usr/bin/python\n",
      "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n",
      "Python 3.10.12\n",
      "pip 23.3.1 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n"
     ]
    }
   ],
   "source": [
    "# Verify the installation\n",
    "!python3 --version\n",
    "!python --version\n",
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==1.18.3 (from -r requirements.txt (line 1))\n",
      "  Downloading datasets-1.18.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting einops==0.4.0 (from -r requirements.txt (line 2))\n",
      "  Downloading einops-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting gpustat==1.1 (from -r requirements.txt (line 3))\n",
      "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hydra-core==1.1.1 (from -r requirements.txt (line 4))\n",
      "  Downloading hydra_core-1.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting higher==0.2.1 (from -r requirements.txt (line 5))\n",
      "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting importlib-metadata==6.3.0 (from -r requirements.txt (line 6))\n",
      "  Downloading importlib_metadata-6.3.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting matplotlib==3.5.1 (from -r requirements.txt (line 7))\n",
      "  Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting nltk==3.6.5 (from -r requirements.txt (line 8))\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy==1.22.1 (from -r requirements.txt (line 9))\n",
      "  Downloading numpy-1.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting omegaconf==2.1.1 (from -r requirements.txt (line 10))\n",
      "  Downloading omegaconf-2.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pandas==1.4.0 (from -r requirements.txt (line 11))\n",
      "  Downloading pandas-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting PyYAML==6.0 (from -r requirements.txt (line 12))\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting scikit-learn==1.0.2 (from -r requirements.txt (line 13))\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting scipy==1.7.3 (from -r requirements.txt (line 14))\n",
      "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting sentence-transformers==3.2.1 (from -r requirements.txt (line 15))\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tokenizers (from -r requirements.txt (line 16))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting torch==2.0.1 (from -r requirements.txt (line 17))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting tqdm==4.62.3 (from -r requirements.txt (line 18))\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl.metadata (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m298.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.44.2 (from -r requirements.txt (line 19))\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m273.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai==0.27.9 (from -r requirements.txt (line 20))\n",
      "  Downloading openai-0.27.9-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting peft==0.7.1 (from -r requirements.txt (line 21))\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting timm==0.9.7 (from -r requirements.txt (line 22))\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m235.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting iopath==0.1.10 (from -r requirements.txt (line 23))\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m202.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opencv-python==4.8.0.76 (from -r requirements.txt (line 24))\n",
      "  Downloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting fairscale==0.4.13 (from -r requirements.txt (line 25))\n",
      "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zhipuai (from -r requirements.txt (line 26))\n",
      "  Downloading zhipuai-2.1.5.20241204-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 27))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting pyarrow!=4.0.0,>=3.0.0 (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting xxhash (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->datasets==1.18.3->-r requirements.txt (line 1)) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0 (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (23.2)\n",
      "Collecting nvidia-ml-py>=11.450.129 (from gpustat==1.1->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat==1.1->-r requirements.txt (line 3)) (5.9.6)\n",
      "Collecting blessed>=1.17.1 (from gpustat==1.1->-r requirements.txt (line 3))\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core==1.1.1->-r requirements.txt (line 4))\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m303.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata==6.3.0->-r requirements.txt (line 6)) (1.0.0)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.5.1->-r requirements.txt (line 7))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.5.1->-r requirements.txt (line 7))\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m329.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib==3.5.1->-r requirements.txt (line 7))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 7)) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 7)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 7)) (2.8.2)\n",
      "Collecting click (from nltk==3.6.5->-r requirements.txt (line 8))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk==3.6.5->-r requirements.txt (line 8))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk==3.6.5->-r requirements.txt (line 8))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m238.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1 (from pandas==1.4.0->-r requirements.txt (line 11))\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.0.2->-r requirements.txt (line 13))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 17)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 17)) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 17)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 17)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 17)) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.44.2->-r requirements.txt (line 19))\n",
      "  Downloading safetensors-0.5.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers (from -r requirements.txt (line 16))\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.7.1->-r requirements.txt (line 21))\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->-r requirements.txt (line 22)) (0.16.0+cu118)\n",
      "Collecting portalocker (from iopath==0.1.10->-r requirements.txt (line 23))\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 17)) (68.2.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 17)) (0.41.3)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading cmake-3.31.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 17))\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting cachetools>=4.2.2 (from zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting httpx>=0.23.0 (from zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0,>=1.9.0 (from zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydantic-core>=2.14.6 (from zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pyjwt<2.9.0,>=2.8.0 (from zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat==1.1->-r requirements.txt (line 3)) (0.2.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from blessed>=1.17.1->gpustat==1.1->-r requirements.txt (line 3)) (1.16.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m303.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai->-r requirements.txt (line 26)) (4.0.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai->-r requirements.txt (line 26)) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai->-r requirements.txt (line 26)) (3.4)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.23.0->zhipuai->-r requirements.txt (line 26))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0 (from datasets==1.18.3->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n"
     ]
    }
   ],
   "source": [
    "# takes ~2 mins\n",
    "!pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0eGk7gM_wg4",
    "outputId": "0f3fc28c-641b-49b9-8983-b8269fca2443"
   },
   "outputs": [],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Pp6wahHlBxHZ",
    "outputId": "6a27c3e2-2e6b-43cc-cf81-23d8b5ad8b27"
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMGZhZ7NmphY"
   },
   "source": [
    "## Import modules & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir /workspace/hf\n",
    "!ls /workspace\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment=\"baseline\"\n",
    "# data=\"./data/tofu_test_dummy_zsre.json\"\n",
    "# data=\"./data/tofu_test_zsre.json\"\n",
    "# data=\"./data/tofu_test_avoidant_zsre.json\"\n",
    "data=\"./data/notebook/zsre_mend_eval_portability_gpt4.json\"\n",
    "metrics_save_dir=f\"/workspace/outputs/MEMIT_{experiment}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For LlaMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import BaseEditor\n",
    "from easyeditor import MEMITHyperParams\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open(data, 'r', encoding='utf-8'))\n",
    "prompts = [test_data_['src'] for test_data_ in test_data]\n",
    "ground_truth = [edit_data_['pred'] for edit_data_ in test_data]\n",
    "rephrase_prompts = [edit_data_['rephrase'] for edit_data_ in test_data]\n",
    "target_new = [edit_data_['alt'] for edit_data_ in test_data]\n",
    "locality_prompts = [edit_data_['loc'] for edit_data_ in test_data]\n",
    "locality_ans = [edit_data_['loc_ans'] for edit_data_ in test_data]\n",
    "\n",
    "locality_inputs = {\n",
    "    'neighborhood':{\n",
    "        'prompt': locality_prompts,\n",
    "        'ground_truth': locality_ans\n",
    "    },\n",
    "}\n",
    "\n",
    "if experiment == \"baseline\":\n",
    "    portability_prompts = [edit_data_['portability']['New Question'] for edit_data_ in test_data]\n",
    "    portability_ans = [edit_data_['portability']['New Answer'] for edit_data_ in test_data]\n",
    "    portability_inputs = {\n",
    "        'one_hop':{\n",
    "            'prompt': portability_prompts,\n",
    "            'ground_truth': portability_ans\n",
    "        },\n",
    "    }\n",
    "\n",
    "subject = [edit_data_['subject'] for edit_data_ in test_data]\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~13m\n",
    "import os\n",
    "\n",
    "hparams=MEMITHyperParams.from_hparams('./hparams/MEMIT/notebook.yaml')\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "\n",
    "sequential_edit = True\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    rephrase_prompts=rephrase_prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    keep_original_weight=False,\n",
    "    sequential_edit=sequential_edit\n",
    ")\n",
    "\n",
    "print('data: ', data)\n",
    "print('save to: ', metrics_save_dir)\n",
    "print('model: ', hparams.model_name)\n",
    "print('sequential_edit: ', sequential_edit)\n",
    "\n",
    "os.makedirs(metrics_save_dir, exist_ok=True)\n",
    "json.dump(metrics, open(os.path.join(metrics_save_dir, f'MEMIT_results.json'), 'w'), indent=4)\n",
    "model_save_dir = os.path.join(metrics_save_dir, 'model')\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "edited_model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model on SuperCloud\n",
    "!ssh-keyscan txe1-login.mit.edu >> ~/.ssh/known_hosts\n",
    "!ssh-keygen -t rsa -b 2048 -f ~/.ssh/id_rsa -N \"\"\n",
    "!cat ~/.ssh/id_rsa.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~8m\n",
    "!apt update -y\n",
    "!apt install rsync -y\n",
    "!rsync -avz -e ssh /workspace/outputs/ shossain@txe1-login.mit.edu:/home/gridsan/shossain/EasyEdit/outputs/ -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do something with stats_dir in hparams?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
