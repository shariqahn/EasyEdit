LlamaTokenizer Detected, Set pad token id and left padding!!!
LlamaTokenizer Detected, Set pad token id and left padding!!!
12/23/2024 16:52:25 - INFO - easyeditor.trainer.BaseTrainer -   Config: SERACTrainingHparams(model_name='./scr/models--locuslab--tofu_ft_llama2-7b/snapshots/8fa500e8f345f1dd9cfe95bb4689878c944c9cbd', model_class='LlamaForCausalLM', small_name='./scr/models--Cheng98--llama-160m/snapshots/aa9998f9aab075589dd4836d903b26501e549e2e', tokenizer_class='LlamaTokenizer', tokenizer_name='./scr/models--locuslab--tofu_ft_llama2-7b/snapshots/8fa500e8f345f1dd9cfe95bb4689878c944c9cbd', cls_name='./scr/models--distilbert--distilbert-base-cased/snapshots/6ea81172465e8b0ad3fddeed32b986cdcdcffcf0', cls_class='AutoModel', inner_params=[], archive=None, alg='SERAC', lr=1e-05, edit_lr=0.01, seed=0, lr_lr=0.0, cedit=0.1, cloc=1.0, cbase=1.0, dropout=0.0, final_eval=True, supervised=False, train_base=False, no_grad_layers=None, soft_weighting=False, checkpoint_grad=False, cross_attend=False, cos=False, freeze=None, square=True, bound_embeds=False, use_all_negatives=False, freeze_cntr=False, dist_heads=1, lora=None, results_dir='./outputs', device='cuda:0', batch_size=10, model_save_pt=1000, edit_bs=1, silent=False, log_interval=1000, val_interval=1000, early_stop_patience=30000, early_stop_key='edit/acc_val', eval_only=False, half=False, save=True, debug=False, log_errors=False, unlikelihood=True, val_batch_size=1, accumulate_bs=10, val_steps=1000, opt='Adam', grad_clip=100.0, exact_match=False, max_epochs=None, max_iters=100000, max_length=32, model_parallel=True)
12/23/2024 16:52:25 - INFO - easyeditor.trainer.models -   Loading model class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> with name ./scr/models--locuslab--tofu_ft_llama2-7b/snapshots/8fa500e8f345f1dd9cfe95bb4689878c944c9cbd
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:22<00:11, 11.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.08s/it]
12/23/2024 16:53:05 - INFO - easyeditor.trainer.models -   Set 0 dropout modules to p=0.0
12/23/2024 16:53:05 - INFO - easyeditor.trainer.BaseTrainer -   Loading class SERAC from module <class 'easyeditor.trainer.algs.SERAC.SERAC'>
/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
12/23/2024 16:53:16 - INFO - easyeditor.trainer.utils -   Set 13 dropout modules to p=0.0
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
12/23/2024 16:53:17 - INFO - easyeditor.trainer.utils -   Set 0 dropout modules to p=0.0
12/23/2024 16:53:17 - INFO - easyeditor.trainer.BaseTrainer -   Building optimizer <class 'torch.optim.adam.Adam'> with lr 1e-05
