LlamaTokenizer Detected, Set pad token id and left padding!!!
LlamaTokenizer Detected, Set pad token id and left padding!!!
12/27/2024 02:42:51 - INFO - easyeditor.trainer.BaseTrainer -   Config: SERACTrainingHparams(model_name='./scr/models--locuslab--tofu_ft_llama2-7b/snapshots/8fa500e8f345f1dd9cfe95bb4689878c944c9cbd', model_class='LlamaForCausalLM', small_name='./scr/models--Cheng98--llama-160m/snapshots/aa9998f9aab075589dd4836d903b26501e549e2e', tokenizer_class='LlamaTokenizer', tokenizer_name='./scr/models--locuslab--tofu_ft_llama2-7b/snapshots/8fa500e8f345f1dd9cfe95bb4689878c944c9cbd', cls_name='./scr/models--distilbert--distilbert-base-cased/snapshots/6ea81172465e8b0ad3fddeed32b986cdcdcffcf0', cls_class='AutoModel', inner_params=[], archive=None, alg='SERAC', lr=1e-05, edit_lr=0.01, seed=0, lr_lr=0.0, cedit=0.1, cloc=1.0, cbase=1.0, dropout=0.0, final_eval=True, supervised=False, train_base=False, no_grad_layers=None, soft_weighting=False, checkpoint_grad=False, cross_attend=False, cos=False, freeze=None, square=True, bound_embeds=False, use_all_negatives=False, freeze_cntr=False, dist_heads=1, lora=None, results_dir='./outputs', device='cuda:0', batch_size=10, model_save_pt=1000, edit_bs=1, silent=False, log_interval=1000, val_interval=1000, early_stop_patience=30000, early_stop_key='edit/acc_val', eval_only=False, half=False, save=True, debug=False, log_errors=False, unlikelihood=True, val_batch_size=1, accumulate_bs=10, val_steps=1000, opt='Adam', grad_clip=100.0, exact_match=False, max_epochs=None, max_iters=100000, max_length=32, model_parallel=True)
12/27/2024 02:42:51 - INFO - easyeditor.trainer.models -   Loading model class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> with name ./scr/models--locuslab--tofu_ft_llama2-7b/snapshots/8fa500e8f345f1dd9cfe95bb4689878c944c9cbd
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.22s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:23<00:11, 11.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00,  9.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.35s/it]
12/27/2024 02:43:40 - INFO - easyeditor.trainer.models -   Set 0 dropout modules to p=0.0
12/27/2024 02:43:40 - INFO - easyeditor.trainer.BaseTrainer -   Loading class SERAC from module <class 'easyeditor.trainer.algs.SERAC.SERAC'>
/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
12/27/2024 02:43:53 - INFO - easyeditor.trainer.utils -   Set 13 dropout modules to p=0.0
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
12/27/2024 02:43:54 - INFO - easyeditor.trainer.utils -   Set 0 dropout modules to p=0.0
12/27/2024 02:43:54 - INFO - easyeditor.trainer.BaseTrainer -   Building optimizer <class 'torch.optim.adam.Adam'> with lr 1e-05
Traceback (most recent call last):
  File "/home/gridsan/shossain/EasyEdit/serac_train.py", line 16, in <module>
    trainer.run()
  File "/home/gridsan/shossain/EasyEdit/easyeditor/trainer/BaseTrainer.py", line 195, in run
    train_info = self.train_step(batch)
  File "/home/gridsan/shossain/EasyEdit/easyeditor/trainer/EditTrainer.py", line 141, in train_step
    l_total, l_edit, l_loc, l_base, info_dict = self.edit_step(
  File "/home/gridsan/shossain/EasyEdit/easyeditor/trainer/EditTrainer.py", line 54, in edit_step
    l_edit = self.model.edit_loss_fn(
  File "/home/gridsan/shossain/EasyEdit/easyeditor/trainer/algs/editable_model.py", line 24, in _edit_loss_fn
    return masked_log_probs(config, pred, targ, shift=True, **kwargs)
  File "/home/gridsan/shossain/EasyEdit/easyeditor/trainer/losses.py", line 135, in masked_log_probs
    return multiclass_log_probs(config, pred, targ, shift=shift, exact_match=exact_match, **kwargs)
  File "/home/gridsan/shossain/EasyEdit/easyeditor/trainer/losses.py", line 70, in multiclass_log_probs
    unmasked_log_probs = pred.log_softmax(-1).gather(-1, targ.unsqueeze(-1)).squeeze(-1)
RuntimeError: Size does not match at dimension 1 expected index [10, 32, 1] to be smaller than self [10, 31, 32000] apart from dimension 2
