Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.61s/it]
Traceback (most recent call last):
  File "/home/gridsan/shossain/EasyEdit/examples/run_grace_editing.py", line 173, in <module>
    model = LlamaForCausalLM.from_pretrained(name).to(f'cuda:{device}')
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2905, in to
    return super().to(*args, **kwargs)
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 31.74 GiB total capacity; 31.29 GiB already allocated; 93.12 MiB free; 31.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
