2024-12-29 17:00:47,834 - easyeditor.editors.editor - INFO - Instantiating model
12/29/2024 17:00:47 - INFO - easyeditor.editors.editor -   Instantiating model
model kwargs:  {'torch_dtype': torch.float32, 'device_map': 'auto', 'load_in_8bit': True}
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.93s/it]
2024-12-29 17:01:25,569 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...
12/29/2024 17:01:25 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...
12/29/2024 17:01:25 - INFO - sentence_transformers.SentenceTransformer -   Use pytorch device_name: cuda
12/29/2024 17:01:25 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: /home/gridsan/shossain/EasyEdit/scr/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9
/home/gridsan/shossain/.conda/envs/easy/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Batches:   0%|          | 0/34 [00:00<?, ?it/s]Batches:   3%|▎         | 1/34 [00:05<02:47,  5.07s/it]Batches:  15%|█▍        | 5/34 [00:05<00:22,  1.28it/s]Batches:  26%|██▋       | 9/34 [00:05<00:09,  2.72it/s]Batches:  41%|████      | 14/34 [00:05<00:03,  5.05it/s]Batches:  56%|█████▌    | 19/34 [00:05<00:01,  7.98it/s]Batches:  71%|███████   | 24/34 [00:05<00:00, 11.57it/s]Batches:  88%|████████▊ | 30/34 [00:05<00:00, 16.57it/s]Batches: 100%|██████████| 34/34 [00:05<00:00,  5.84it/s]
  0%|          | 0/40 [00:00<?, ?it/s]/home/gridsan/shossain/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|▎         | 1/40 [00:02<01:25,  2.20s/it]  5%|▌         | 2/40 [00:03<01:09,  1.82s/it]  8%|▊         | 3/40 [00:05<01:04,  1.75s/it] 10%|█         | 4/40 [00:07<01:01,  1.71s/it] 12%|█▎        | 5/40 [00:08<00:57,  1.65s/it] 15%|█▌        | 6/40 [00:10<00:57,  1.69s/it] 18%|█▊        | 7/40 [00:12<00:55,  1.67s/it] 20%|██        | 8/40 [00:13<00:55,  1.73s/it] 22%|██▎       | 9/40 [00:15<00:57,  1.84s/it] 25%|██▌       | 10/40 [00:17<00:54,  1.83s/it] 28%|██▊       | 11/40 [00:19<00:52,  1.81s/it] 30%|███       | 12/40 [00:21<00:50,  1.79s/it] 32%|███▎      | 13/40 [00:23<00:50,  1.87s/it] 35%|███▌      | 14/40 [00:25<00:49,  1.89s/it] 38%|███▊      | 15/40 [00:27<00:47,  1.89s/it] 40%|████      | 16/40 [00:29<00:45,  1.90s/it] 42%|████▎     | 17/40 [00:30<00:42,  1.84s/it]slurmstepd-d-14-11-1: error: *** JOB 27685387 ON d-14-11-1 CANCELLED AT 2024-12-29T17:02:06 ***
